SENSE_QUERIES = {
    "èµ°": {
        "walk": [
            "èµ° åœ¨",
            "èµ° åˆ°",
            "æ…¢æ…¢ èµ°",
            "èµ° è¿‡å»",
            "èµ° è¿‡æ¥",
            "èµ° è·¯",
            "å¾€ å‰ èµ°",
            "å¾€ å èµ°",
            "èµ° ä¸€ æ­¥",
            "èµ° å‡  æ­¥",
            "èµ° ä¸€ æ®µ",
            "èµ° å¾ˆ è¿œ",
            "èµ° ä¸ åŠ¨",
            "èµ° å¾— å¿«",
            "èµ° å¾— æ…¢",
            "èµ° ä¸‹å»",
            "èµ° ä¸Šæ¥",
            "èµ° å‡ºå»",
            "èµ° è¿›å»"
        ],
        "leave": [
            "äºº èµ° äº†",
            "æ—¶é—´ èµ°",
            "æµç¨‹ èµ°",
            "äº‹æƒ… èµ°"
        ],
        "social_contact": [
            "èµ° äº²æˆš",
            "èµ° è®¿",
            "èµ° é‚»é‡Œ",
            "èµ° è¡— ä¸² å··",
            "èµ° åŠ¨",
            "èµ° åé—¨",
            "èµ° å…³ç³»"
        ],
        "transmit (interchangeable_operation)": [
            "èµ° ç§",
            "èµ° ä¿¡",
            "èµ° è´¦",
            "èµ° è´§",
            "èµ° é•–",
            "èµ° æ°´",
            "èµ° å’",
            "èµ° é©¬ç¯"
            
        ],
        "lose": [
            "èµ° å‘³",
            "èµ° å½¢",
            "èµ° æ ·",
            "èµ° æ¼",
            "èµ° é£",
            "èµ° æ°”",
            "èµ° è°ƒ"
            
        ]
    },
    "åƒ": {
        "eat": [
            "åƒ é¥­",
            "åƒ èœ",
            "åƒ é¤",
            "åƒ è‚‰",
            "åƒ ä¸œè¥¿",
            "åƒ é£Ÿç‰©",
            "åƒ æ—©é¥­",
            "åƒ åˆé¥­",
            "åƒ æ™šé¥­",
            "åƒ ç‚¹ å¿ƒ",
            "åƒ é›¶é£Ÿ",
            "åƒ ç«é”…",
            "åƒ çƒ§çƒ¤",
            "åƒ é¢",
            "åƒ ç±³é¥­",
            "åƒ ä¸€å£",
            "åƒ å‡ å£",
            "åƒ å¾— é¥±",
            "åƒ ä¸ ä¸‹"
        ],
        "suffer": [
            "åƒ äº",
            "åƒ è‹¦",
            "åƒ å®˜å¸",
            "åƒ é—­é—¨ç¾¹",
            "åƒ åŠ›",
            "åƒ ä¸æ¶ˆ",
            "åƒ è´¥ä»—",
            "åƒ å¤„åˆ†",
            "åƒ æƒ©ç½š",
            "åƒ ç½šå•",
            "åƒ æ‰“",
            "åƒ éª‚",
            "åƒ è´£éª‚",
            "åƒ æ‰¹è¯„",
            "åƒ å§”å±ˆ",
            "åƒ ç½ª",
            "åƒ è‹¦å¤´",
            "åƒ äº ä¸Š äº"
        ],
        "feel": [
            "åƒ æƒŠ",
            "åƒ ç´§",
            "åƒ å ‘",
            "åƒ é†‹",
            "åƒ ç˜ª",
            "åƒ å‡†"
        ],
        "eliminate (merger, swallow_up)": [
            "åƒ ä¸€ä¸ª è¿",
            "åƒ å­",
            "é€šåƒ",
            "é»‘ åƒ é»‘",
            "åƒ åˆ©æ¯",
            "åƒ ç©ºé¥·",
            "åƒ å›æ‰£",
            "åƒ å·®ä»·",
            "åƒ çº¢åˆ©",
            "åƒ é¡¹ç›®",
            "åƒ å¯¹æ–¹",
            "åƒ æ‰",
            "å åƒ",
            "è¢« åƒ æ‰",
            "å¤§é±¼ åƒ å°é±¼",
            "å¼ºè€… åƒ å¼±è€…"
        ]
    },
    "å¼€": {
        "open": [
            "å¼€ é—¨",
            "å¼€ ç¯",
            "å¼€ ç”µè„‘",
            "å¼€ ç³»ç»Ÿ",
            "å¼€ çª—",
            "å¼€ å†°ç®±",
            "å¼€ ç”µæº",
            "å¼€ ç”µç¯",
            "å¼€ ç…§æ˜",
            "å¼€ ç”µè§†",
            "å¼€ ç©ºè°ƒ",
            "å¼€ æ”¶éŸ³æœº",
            "å¼€ æ°´é¾™å¤´",
            "å¼€ ç“¶å­",
            "å¼€ åŒ…",
            "å¼€ ç›’å­",
            "å¼€ è½¦é—¨"
        ],
        "open_in_mind": [
            "å¼€ çœ¼",
            "å¼€çª",
            "å¼€æ˜",
            "å¼€æœ—",
            "æƒ³ å¼€",
            "çœ‹ å¼€",
            "å¿ƒ å¼€",
            "èƒ¸ å¼€",
            "å¿ƒèƒ¸ å¼€é˜”",
            "çœ¼ç•Œ å¼€é˜”",
            "æ€è·¯ å¼€é˜”",
            "å¼€ èƒ¸æ€€",
            "å¼€ æ€è·¯",
            "å¼€ æ€€"
        ],
        "seperate": [
            "åŠˆå¼€",
            "å‰²å¼€",
            "ä¸‰ä¸ƒ å¼€",
            "äº”äº” å¼€",
            "åˆ‡å¼€",
            "å¯¹ å¼€",
            "å‰–å¼€",
            "æ‹†å¼€",
            "åˆ†å¼€",
            "å¼€ åˆ†",
            "å¼€ è£‚",
            "å¼€ ç¼",
            "åˆ‡ åˆ†",
            "å¼€ åˆ†è£‚"

        ],
        "enlarge": [
            "å¼€ æ‰©",
            "å¼€ æ‹“",
            "å¼€ è¾Ÿ",
            "å¼€ ç–†",
            "å¼€å‘",
            "å¼€ é‡‡",
            "å¼€ å‘ åŒº",
            "å¼€ ç–† è¾Ÿ åœŸ",
            "å¼€ æ‹“ å¸‚åœº",
            "å¼€ å‘ é¡¹ç›®",
            "å¼€ é‡‡ çŸ¿äº§",
            "å¼€ å‘ åŒºåŸŸ",
            "å¼€ åˆ›",
            "å¼€ å»º",
            "å¼€ æ˜",
            "å¼€ å¯ æ–°å±€",
            "å¼€ æ”¾"
        ],
        "cut_through": [
            "å¼€ é€š",
            "å¼€ æ¸ ",
            "å¼€ æ²³",
            "å¼€ éš§é“",
            "å¼€ æ´",
            "å¼€ è·¯",
            "å¼€ æ¡¥",
            "å¼€ é“",
            "å¼€ æ°´æ¸ ",
            "å¼€ ä¸‹æ°´é“",
            "å¼€ é€šé“",
            "å¼€ è¡¢é“",
            "å¼€ æ¸¯å£",
            "å¼€ ç®¡é“"
        ]
    },
    "æ‹¿": {
        "take": [
            "æ‹¿ æ‰‹æœº",
            "æ‹¿ ä¸œè¥¿",
            "æ‹¿ ä¹¦",
            "æ‹¿ åŒ…",
            "æ‹¿ é’¥åŒ™",
            "æ‹¿ é’±åŒ…",
            "æ‹¿ ç¬”",
            "æ‹¿ æ¯å­",
            "æ‹¿ æ¯æ°´",
            "æ‹¿ ç¤¼ç‰©",
            "æ‹¿ ææ–™",
            "æ‹¿ èµ„æ–™",
            "æ‹¿ é›¶é£Ÿ",
            "æ‹¿ é£Ÿç‰©",
            "æ‹¿ ç¥¨",
            "æ‹¿ æ‰‹æœºç…§"
        ],
        "control": [
            "æ‹¿ ä¸»ä¹‰",
            "æ‹¿ æƒ",
            "æ‹¿ æ‰‹",
            "å æ‹¿ ä¹ ç¨³",
            "æ‹¿ å¾—ä¸‹",
            "æ‹¿ å‡†",
            "æ‹¿ ä¸»æ„",
            "æ‹¿ è®¡åˆ’",
            "æ‹¿ å†³å®š",
            "æ‹¿ ç­–ç•¥",
            "æ‹¿ ä¸»æƒ",
            "æ‹¿ æŠŠæ¡",
            "æ‹¿ å‘½ä»¤",
            "æ‹¿ å‡†æ˜Ÿ",
            "æ‹¿ åº•ç‰Œ"
        ],
        "coerce": [
            "æ‹¿æ",
            "æ‹¿å¤§",
            "æ‹¿çŸ­å„¿",
            "æ‹¿æŠŠ",
            "æ‹¿ä¹”",
            "æ‹¿æˆ‘",
            "æ‹¿ è…” ä½œ åŠ¿",
            "æ‹¿ è…” æ‹¿ è°ƒ",
            "æ‹¿ è§’åº¦",
            "æ‹¿ è§„çŸ©",
            "æ‹¿ è®¡ç­–",
            "æ‹¿ æ‹›",
            "æ‹¿ æŠ€å·§",
            "æ‹¿ èŠ‚å¥",
            "æ‹¿ åˆ†å¯¸",
            "æ‹¿ æ‰‹æ®µ"
        ],
        "arrest": [
            "æ‹¿ è·",
            "æ“’ æ‹¿",
            "æ‹¿é—®",
            "ç¼‰ æ‹¿",
            "æ‰ æ‹¿",
            "è¿½ æ‹¿",
            "æŠ“ æ‹¿",
            "é€® æ‹¿",
            "æ‹˜ æ‹¿",
            "æŠ¼ æ‹¿",
            "æŸ¥ æ‹¿",
            "æ”¶ æ‹¿",
            "æˆª æ‹¿"

        ],
        "occupy": [
            "æ‹¿ä¸‹ é¡¹ç›®",
            "æ‹¿ä¸‹ å®¢æˆ·",
            "æ‹¿ä¸‹ å¸‚åœº",
            "æ‹¿ä¸‹ è®¢å•",
            "æ‹¿ä¸‹ åˆåŒ",
            "æ‹¿ä¸‹ æ¯”èµ›",
            "æ‹¿ä¸‹ è®°å½•",
            "æ‹¿ä¸‹ ç¢‰å ¡",
            "æ‹¿ä¸‹ é«˜åœ°",
            "æ‹¿ä¸‹ é˜µåœ°",
            "æ‹¿ä¸‹ åŸ",
            "æ”» æ‹¿",
            "æ‹¿ä¸‹ å† å†›",
            "æ‹¿ä¸‹ å†³èµ›",
            "æ‹¿ä¸‹ å†›äº‹ç›®æ ‡",
            "æ‹¿ä¸‹ æˆæœ",
            "æ‹¿ä¸‹ åˆä½œ",
            "æ‹¿ä¸‹ æŠ•æ ‡",
            "æ‹¿ä¸‹ è£èª‰",
            "æ‹¿ä¸‹ åæ¬¡"
        ]
    }
}

import csv
import html
import re
import requests
import time
from collections import defaultdict
import os
import os
'''
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CSV_PATH = os.path.join(BASE_DIR, "weakly_supervised_polysemy_sentences.csv")
'''

output_file = "weakly_supervised_polysemy_sentences.csv"

file_exists = os.path.exists(output_file)


os.environ["HTTP_PROXY"] = ""
os.environ["HTTPS_PROXY"] = ""

MAX_PER_SENSE = 80

def load_existing_counts(csv_path):    #è¿™ä¸ªcsv_pathæ˜¯ä¸æ˜¯åé¢è¦æ”¹
    """
    è¿”å›ä¸€ä¸ª dict:
    {(word, sense): count}
    """
    counts = defaultdict(int)

    if not os.path.exists(csv_path):
        return counts

    with open(csv_path, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            key = (row["word"], row["sense"])
            counts[key] += 1

    return counts


def clean_and_trim_snippet(snippet, word, min_len=12, max_len=50, context=0):

    if not snippet:
        return None

    # 1ï¸âƒ£ HTML åè½¬ä¹‰
    text = html.unescape(snippet)

    # 2ï¸âƒ£ å»æ‰å¤šä½™ç©ºç™½ / æ¢è¡Œ
    text = re.sub(r"\s+", "", text)

    # 3ï¸âƒ£ å¿«é€Ÿè¿‡æ»¤æ˜æ˜¾æ— å…³å†…å®¹
    BAD_PATTERNS = [
        "ç‚¹å‡»", "ç™»å½•", "æ³¨å†Œ", "ç‰ˆæƒæ‰€æœ‰", "å¹¿å‘Š",
        "æ›´å¤š", "é˜…è¯»å…¨æ–‡", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ","åƒé¸¡"
    ]
    if any(bad in text for bad in BAD_PATTERNS):
        return None

    # 4ï¸âƒ£ å¿…é¡»åŒ…å«ç›®æ ‡å¤šä¹‰å­—
    if word not in text:
        return None

    # 5ï¸âƒ£ æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ‡åˆ†å¥å­
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
    sentences = [s.strip() for s in sentences if s.strip()]

    for idx, sent in enumerate(sentences):
        if word not in sent:
            continue

        # 6ï¸âƒ£ ä¸¢æ‰æ˜æ˜¾æ ‡é¢˜ / æ®‹å¥
        if any(x in sent for x in ["ã€Š", "ã€‹", "Â·", "|"]):
            continue

        # 7ï¸âƒ£ é•¿åº¦å¼ºè¿‡æ»¤
        if len(sent) < min_len or len(sent) > max_len:
            continue

        # 8ï¸âƒ£ å¯é€‰ä¸Šä¸‹æ–‡
        if context > 0:
            start = max(0, idx - context)
            end = min(len(sentences), idx + context + 1)
            sent_with_context = "ã€‚".join(sentences[start:end]) + "ã€‚"
            return sent_with_context

        # è¿”å›å•å¥
        return sent + "ã€‚"

    # æ²¡æ‰¾åˆ°åˆé€‚å¥å­å°±è¿”å› None
    return None



import requests

SERPAPI_KEY = "6df53d617be540759e19e512ac4a8ec9d1ec7148618b0239f9fbb564b85c3cd7"

'''
def search_bing_serpapi(query, page=0):
    url = "https://serpapi.com/search.json"
    params = {
        "engine": "bing",        # ğŸ‘ˆ å…³é”®å°±åœ¨è¿™ä¸€è¡Œ
        "q": query,
        "api_key": SERPAPI_KEY,
        "count": 10,             # æ¯é¡µè¿”å›æ•°é‡
        "first": page * 10,      # Bing é£æ ¼åˆ†é¡µ
        "mkt": "zh-CN"           # ä¸­æ–‡ï¼›è‹±æ–‡ç”¨ en-US
    }

    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    return r.json()
'''


def search_bing_serpapi(query, page=0, retries=5, timeout=60):
    for attempt in range(retries):
        try:
            response = requests.get(
                "https://serpapi.com/search.json",
                params={
                    "engine": "bing",
                    "q": query,
                    "api_key": SERPAPI_KEY,
                    "count": 10,
                    "first": page*10,
                    "mkt": "zh-CN"
                },
                timeout=timeout
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.ReadTimeout as e:
            print(f"[TIMEOUT] {e}, retry {attempt+1}/{retries}")
            time.sleep(2)
        except requests.exceptions.ConnectionError as e:
            print(f"[Connection ERROR] {e}, retry {attempt+1}/{retries}")
            time.sleep(2)
        except requests.exceptions.SSLError as e:
            print(f"[SSL ERROR] {e}, retry {attempt+1}/{retries}")
            time.sleep(2)
    print(f"[FAILED] query={query} page={page}")
    return None


    # ---- SerpAPI / Bing å…³é”®é˜²æŠ¤ ----
    if r.status_code != 200:
        print("HTTP ERROR (SerpAPI Bing):", r.status_code)
        print(r.text[:200])
        return None

    try:
        return r.json()
    except Exception as e:
        print("JSON ERROR (SerpAPI Bing)")
        print(r.text[:200])
        return None
    from snownlp import SnowNLP  # ä¸­æ–‡å¥å­åˆ†å‰²åº“ï¼Œpip install snownlp


def extract_sentences_with_context(snippet, target_word, context=1):
    # ç²—ç•¥ä¸­æ–‡å¥å­åˆ†å‰²
    sents = re.split(r'(ã€‚|ï¼|\!|ï¼Ÿ|\?)', snippet)
    # åˆå¹¶åˆ†å‰²ç¬¦
    sentences = []
    for i in range(0, len(sents)-1, 2):
        sentences.append(sents[i] + sents[i+1])

    # æ‰¾åŒ…å« target_word çš„ç´¢å¼•
    idx_list = [i for i, s in enumerate(sentences) if target_word in s]
    result = []
    for idx in idx_list:
        start = max(idx - context, 0)
        end = min(idx + context + 1, len(sentences))
        sentence_with_context = "".join(sentences[start:end])
        if len(sentence_with_context) >= 8:  # å¼±è¿‡æ»¤
            result.append(sentence_with_context)
    return result

def append_to_csv(csv_path, word, sense, sentences):
    file_exists = os.path.exists(csv_path)

    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)

        if not file_exists:
            writer.writerow(["word", "sense", "sentence"])

        for sent in sentences:
            writer.writerow([word, sense, sent])





def collect_sentences(query, pages=9):
    snippets = []

    for i in range(pages):
        data = search_bing_serpapi(query, page=i)
        if not data or "organic_results" not in data:
            continue

        for item in data["organic_results"]:
            snippet = item.get("snippet", "")
            if snippet:
                snippets.append(snippet)

        time.sleep(2)

    return snippets



def collect_sense_sentences(
    word,
    sense,
    queries,
    target_n,
    writer,
    csv_file,
    pages=9,
    collected=None,
    context=1,
    
):

    """
    - target_n: æœ¬æ¬¡è¿™ä¸ª sense è¿˜éœ€è¦è¡¥æŠ“å¤šå°‘æ¡
    - csv_path: CSV æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè¾¹æŠ“è¾¹å†™ï¼‰
    """
    if collected is None:
        collected=set()
    

    for q in queries:
        if len(collected) >= target_n:
            break

        try:
            results = collect_sentences(q, pages=pages)
        except Exception as e:
            print(f"[ERROR] {word} | {sense} | query='{q}'")
            print(e)
            continue   # âœ… ä¿ç•™ï¼šå‡ºé”™ç›´æ¥ç»“æŸå½“å‰ senseï¼Œä½†å·²å†™çš„ä¸ä¸¢


        for sent in results:
            cleaned = clean_and_trim_snippet(sent, word)

            if cleaned and cleaned not in collected:
                collected.add(cleaned)

                # âœ… 1ï¸âƒ£ ç«‹åˆ»åœ¨ç»ˆç«¯è¾“å‡ºï¼ˆä½ ç°åœ¨å°±èƒ½çœ‹åˆ°ï¼‰
                print(f"[{word} | {sense}] {cleaned}")
      
                # âœ… 2ï¸âƒ£ ç«‹åˆ»å†™å…¥ CSVï¼ˆæ ¸å¿ƒï¼šé˜² 429ï¼‰
                writer.writerow([word, sense, cleaned])
                csv_file.flush()

        
        
            if len(collected) >= target_n:
                break

    return list(collected)




output_file = "weakly_supervised_polysemy_sentences.csv"



def summarize_csv(csv_path):
    stats = defaultdict(int)

    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            stats[(row["word"], row["sense"])] += 1

    print("\n=== Dataset Statistics (per sense) ===")
    for (word, sense), count in sorted(stats.items()):
        print(f"{word}\t{sense}\t{count}")

'''
if __name__ == "__main__":
    CSV_PATH = "weakly_supervised_polysemy_sentences.csv"
    TARGET_PER_SENSE = 80

    existing_counts = load_existing_counts(CSV_PATH)

    for word, senses in SENSE_QUERIES.items():
        for sense, queries in senses.items():
            already = existing_counts.get((word, sense), 0)

            if already >= TARGET_PER_SENSE:
                print(f"SKIP: {word} | {sense} ({already})")
                continue

            need = TARGET_PER_SENSE - already
            print(f"Collecting: {word} | {sense} (need {need})")

            collect_sense_sentences(
                word=word,
                sense=sense,
                queries=queries,
                target_n=need,
                csv_path=CSV_PATH
            )

    # âœ… ç¨‹åºè·‘å®Œåï¼Œå†ç»Ÿä¸€åšç»Ÿè®¡
    summarize_csv(CSV_PATH)
'''

if __name__ == "__main__":
    import os
    import csv

    CSV_PATH = "weakly_supervised_polysemy_sentences.csv"
    TARGET_PER_SENSE = 80  # æ¯ä¸ª sense å¸Œæœ›æ”¶é›†çš„æ€»ä¾‹å¥æ•°

    # -------- è¯»å– CSVï¼Œç»Ÿè®¡å·²æœ‰ä¾‹å¥æ•° --------
    def load_existing_counts(csv_path):
        counts = {}
        if os.path.exists(csv_path):
            with open(csv_path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    key = (row["word"], row["sense"])
                    counts[key] = counts.get(key, 0) + 1
        return counts

    existing_counts = load_existing_counts(CSV_PATH)

    file_exists = os.path.exists(CSV_PATH)

    csv_file = open(CSV_PATH, "a", newline="", encoding="utf-8")
    writer = csv.writer(csv_file)

    if not file_exists:
        writer.writerow(["word", "sense", "sentence"])
        csv_file.flush()


    # -------- æ˜¾ç¤ºæ¯ä¸ª sense å½“å‰æ•°é‡ --------
    print("=== å½“å‰ CSV ä¸­æ¯ä¸ª sense çš„ä¾‹å¥æ•°é‡ ===")
    for (word, sense), count in existing_counts.items():
        print(f"{word} | {sense} : {count} æ¡")
    print("========================================\n")

    # -------- ä¸»åŠ¨è®¾ç½®è¦æ”¶é›†çš„ word / sense / queries --------
    # æ¯æ¬¡æ”¶é›†ä¸€ä¸ª word çš„ä¸€ä¸ª sense
    word = "æ‹¿"
    sense = "occupy"
    queries =      [
            "å¼€ é—¨",
            
            
            "å¼€ çª—",
            "å¼€ å†°ç®±",
            "å¼€ ç”µæº",
            "å¼€ ç”µç¯",
            "å¼€ ç…§æ˜",
            "å¼€ ç”µè§†",
            "å¼€ ç©ºè°ƒ",
            "å¼€ æ”¶éŸ³æœº",
            "å¼€ æ°´é¾™å¤´",
            "å¼€ ç“¶å­",
            "å¼€ åŒ…",
            "å¼€ ç›’å­",
            "å¼€ è½¦é—¨"
        ]
    
    max_collect = 70  # æœ¬æ¬¡å¸Œæœ›æ”¶é›†çš„ä¾‹å¥æ•°é‡ï¼ˆæ‰‹åŠ¨è®¾ç½®ï¼‰"èµ°": {
    pages=9

    already = existing_counts.get((word, sense), 0)
    remaining_needed = max(TARGET_PER_SENSE - already, 0)  # ç»ˆæ­¢æ¡ä»¶

    if remaining_needed == 0:
        print(f"SKIP: {word} | {sense} å·²æœ‰ {already} æ¡ä¾‹å¥ï¼Œè¾¾åˆ°ç›®æ ‡æ•°é‡")
    else:
        # å®é™…æ”¶é›†æ•°é‡ä¸èƒ½è¶…è¿‡å‰©ä½™éœ€è¦çš„æ•°é‡
        collect_limit = min(max_collect, remaining_needed)
        print(f"Collecting: {word} | {sense} (è¿˜éœ€ {remaining_needed} æ¡ï¼Œæœ¬æ¬¡æ”¶é›†ä¸Šé™ {collect_limit})")

        collected = collect_sense_sentences(
            word=word,
            sense=sense,
            queries=queries,
            csv_file=csv_file,
            target_n=collect_limit,
            context=1,  # ä¸Šä¸‹æ–‡å¥æ•°ï¼Œå¯è°ƒ
            pages=9,
            writer=writer
        )
        print(f"æœ¬æ¬¡æ”¶é›†å®Œæˆ: {len(collected)} æ¡")

        

    # -------- æ”¶é›†å®Œæˆåï¼Œç»Ÿè®¡ CSV ä¸­æ¯ä¸ª sense çš„æ€»æ•° --------
    final_counts = load_existing_counts(CSV_PATH)
    print("\n=== CSV ä¸­æ¯ä¸ª sense çš„æœ€ç»ˆæ•°é‡ ===")
    for (w, s), count in final_counts.items():
        print(f"{w} | {s} : {count} æ¡")
    print("====================================")

    csv_file.close()
    